{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, \\\n",
    "    DataCollatorWithPadding, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_path_train = 'data/EXIST2021_dataset/training/EXIST2021_training.tsv'\n",
    "exist_path_test = 'data/EXIST2021_dataset/test/EXIST2021_test_labeled.tsv'\n",
    "\n",
    "results_path = 'data/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-009c7012421518b0\n",
      "Found cached dataset csv (/Users/jacoboj.pedrosa/.cache/huggingface/datasets/csv/default-009c7012421518b0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 2/2 [00:00<00:00, 135.29it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": exist_path_train,\n",
    "                        \"test\": exist_path_test}, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6977/6977 [00:02<00:00, 2649.85ex/s]\n",
      "100%|██████████| 4368/4368 [00:00<00:00, 7476.00ex/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: {\"label\":0} if x == 'non-sexist' else {\"label\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:00<00:00, 17.94ba/s]\n",
      " 80%|████████  | 4/5 [00:00<00:00, 37.32ba/s]\n"
     ]
    }
   ],
   "source": [
    "es_dataset = dataset.filter(lambda example: example[\"language\"] == 'es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo dccuchile/bert-base-spanish-wwm-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'dccuchile/bert-base-spanish-wwm-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 310/310 [00:00<00:00, 73.6kB/s]\n",
      "Downloading: 100%|██████████| 650/650 [00:00<00:00, 178kB/s]\n",
      "Downloading: 100%|██████████| 248k/248k [00:00<00:00, 556kB/s] \n",
      "Downloading: 100%|██████████| 486k/486k [00:00<00:00, 952kB/s] \n",
      "Downloading: 100%|██████████| 134/134 [00:00<00:00, 56.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 86%|████████▌ | 6/7 [00:01<00:00,  3.18ba/s]\n",
      " 80%|████████  | 4/5 [00:00<00:00,  4.19ba/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenizer(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m----> 7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[43mresults_path\u001b[49m,\n\u001b[1;32m      8\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      9\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#fp16=True,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_path' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_path,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=15,\n",
    "    #fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
